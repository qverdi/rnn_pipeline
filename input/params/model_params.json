{
  "activation": ["relu", "sigmoid", "tanh"],
  "batch_size": [8, 16, 32, 64, 128],
  "dropout_rate": [0.1, 0.2, 0.3],
  "learning_rate": [0.0001],
  "layers": ["rnn", "lstm", "gru"],
  "num_layers": [1, 2, 3],
  "neurons": [8, 16, 32, 64],
  "optimizer": ["adam", "sgd", "rmsprop"]
}
